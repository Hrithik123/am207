{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Fall 2018**<br>\n",
    "**Instructors: Rahul Dave**<br>\n",
    "**Due Date: ** Saturday, October 6th, 2018 at 11:59pm\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Upload your final answers in the form of a Jupyter notebook containing all work to Canvas.\n",
    "\n",
    "- Structure your notebook and your work to maximize readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborators\n",
    "\n",
    "Jackie Chea, Dianne Lee, Grace Young"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"answer-separator\">\n",
    "------------------------\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Rubber Chickens Bawk Bawk!\n",
    "\n",
    "In the competitive rubber chicken retail market, the success of a company is built on satisfying the exacting standards of a consumer base with refined and discriminating taste. In particular, customer product reviews are all important. But how should we judge the quality of a product based on customer reviews?\n",
    "\n",
    "On Amazon, the first customer review statistic displayed for a product is the ***average rating***. The following are the main product pages for two competing rubber chicken products, manufactured by Lotus World and Toysmith respectively:\n",
    "\n",
    "\n",
    "Lotus World |  Toysmith\n",
    "- |  - \n",
    "![alt](https://am207.github.io/2018fall/homeworks/lotus1.png) |  ![alt](https://am207.github.io/2018fall/homeworks/toysmith1.png)\n",
    "\n",
    "Clicking on the 'customer review' link on the product pages takes us to a detailed break-down of the reviews. In particular, we can now see the number of times a product is rated a given rating (between 1 and 5 stars).\n",
    "\n",
    "Lotus World |  Toysmith\n",
    "- |  - \n",
    "![alt](https://am207.github.io/2018fall/homeworks/lotus2.png) |  ![alt](https://am207.github.io/2018fall/homeworks/toysmith2.png)\n",
    "\n",
    "(The images above are also included on canvas in case you are offline, see below)\n",
    "\n",
    "In the following, we will ask you to compare these two products using the various rating statistics. **Larger versions of the images are available in the data set accompanying this notebook**.\n",
    "\n",
    "Suppose that for each product, we can model the probability of the value each new rating as the following vector:\n",
    "$$\n",
    "\\theta = [\\theta_1, \\theta_2, \\theta_3, \\theta_4, \\theta_5]\n",
    "$$\n",
    "where $\\theta_i$ is the probability that a given customer will give the product $i$ number of stars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1. Suppose you are told that customer opinions are very polarized in the retail world of rubber chickens, that is, most reviews will be 5 stars or 1 stars (with little middle ground). Choose an appropriate Dirichlet prior for $\\theta$. Recall that the Dirichlet pdf is given by:\n",
    "$$\n",
    "f_{\\Theta}(\\theta) = \\frac{1}{B(\\alpha)} \\prod_{i=1}^k \\theta_i^{\\alpha_i - 1}, \\quad B(\\alpha) = \\frac{\\prod_{i=1}^k\\Gamma(\\alpha_i)}{\\Gamma\\left(\\sum_{i=1}^k\\alpha_i\\right)},\n",
    "$$\n",
    "where $\\theta_i \\in (0, 1)$ and $\\sum_{i=1}^k \\theta_i = 1$, $\\alpha_i > 0 $ for $i = 1, \\ldots, k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "$$\\theta \\sim \\text{Dir}(2,1,1,1,2)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2. Write an expression for the posterior pdf, using a using a multinomial model for observed ratings. Recall that the multinomial pdf is given by:\n",
    "$$\n",
    "f_{\\mathbf{X}\\vert  \\Theta}(\\mathbf{x}) = \\frac{n!}{x_1! \\ldots x_k!} \\theta_1^{x_1} \\ldots \\theta_k^{x_k}\n",
    "$$\n",
    "where $n$ is the total number of trials, $\\theta_i$ is the probability of event $i$ and $\\sum_i \\theta_i = 1$, and $x_i$ is count of outcome $i$ and $\\sum_i x_i = n$. \n",
    "\n",
    "  **Note:** The data you will need in order to define the likelihood function should be read off the image files included in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "Do I need to show how this is calculated? Or just say I know that it's just alpha + sum of occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lotus_percentages = [0.67, 0.17, 0.06, 0.04, 0.06]\n",
    "lotus_counts = [round(162*x) for x in lotus_percentages]\n",
    "lotus_counts = lotus_counts[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "toysmith_percentages = [0.60, 0.11, 0.07, 0.08, 0.14]\n",
    "toysmith_counts = [round(410*x) for x in toysmith_percentages]\n",
    "toysmith_counts = toysmith_counts[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'lotus' : pd.Series(lotus_counts, index=[1,2,3,4,5]),\n",
    "         'toysmith' : pd.Series(toysmith_counts, index=[1,2,3,4,5])}\n",
    "counts = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lotus</th>\n",
       "      <th>toysmith</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>109</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lotus  toysmith\n",
       "1     10        57\n",
       "2      6        33\n",
       "3     10        29\n",
       "4     28        45\n",
       "5    109       246"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3. Sample 1,000 values of $\\theta$ from the *posterior distribution*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prior = pd.Series([2,1,1,1,2], index=[1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate alphas\n",
    "alphas_lotus = (prior + counts.lotus).tolist()\n",
    "alphas_toysmith = (prior + counts.toysmith).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04398014,  0.0513976 ,  0.05176444,  0.17425243,  0.67860538],\n",
       "       [ 0.07992856,  0.05382489,  0.08084951,  0.15101083,  0.63438622],\n",
       "       [ 0.04993223,  0.03403646,  0.04973134,  0.20739791,  0.65890206],\n",
       "       ..., \n",
       "       [ 0.08590423,  0.06909498,  0.10850973,  0.15809992,  0.57839114],\n",
       "       [ 0.06842573,  0.02294416,  0.03829389,  0.18698493,  0.6833513 ],\n",
       "       [ 0.06227224,  0.05780303,  0.05527811,  0.15338853,  0.67125809]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_lotus = np.random.dirichlet(alphas_lotus,size=1000)\n",
    "post_lotus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12623745,  0.09187836,  0.11353768,  0.12293655,  0.54540997],\n",
       "       [ 0.141772  ,  0.08371711,  0.07060051,  0.13876778,  0.56514259],\n",
       "       [ 0.12363229,  0.10697237,  0.0861444 ,  0.09668596,  0.58656498],\n",
       "       ..., \n",
       "       [ 0.16430861,  0.06909896,  0.05900809,  0.12014593,  0.58743843],\n",
       "       [ 0.13146192,  0.07606329,  0.05448908,  0.12130838,  0.61667732],\n",
       "       [ 0.13951888,  0.07206983,  0.08304439,  0.0978578 ,  0.6075091 ]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_toysmith = np.random.dirichlet(alphas_toysmith,size=1000)\n",
    "post_toysmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4. Sample 1,000 values of $x$ from the *posterior predictive distribution*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "object too deep for desired array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-07214e62f479>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpost_lotus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.multinomial (numpy\\random\\mtrand\\mtrand.c:37503)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: object too deep for desired array"
     ]
    }
   ],
   "source": [
    "for i in range(1,len(post_lotus))\n",
    "np.random.multinomial(1000, post_lotus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5. Name at least two major potential problems with using only the average customer ratings to compare products.\n",
    "\n",
    "  (**Hint:** if product 1 has a higher average rating than product 2, can we conclude that product 1 is better liked? If product 1 and product 2 have the same average rating, can we conclude that they are equally good?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.6. Using the samples from your *posterior distribution*, determine which rubber chicken product is superior. Justify your conclusion with sample statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.7. Using the samples from your *posterior predictive distribution*, determine which rubber chicken product is superior. Justify your conclusion with sample statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.8. Finally, which rubber chicken product is superior?\n",
    "\n",
    "  (**Note:** we're not looking for \"the correct answer\" here, any sound decision based on a statistically correct interpretation of your model will be fine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: He Who is Not Courageous Enough to Take Risks Will Accomplish Nothing In Life\n",
    "\n",
    "**No Coding required**\n",
    "\n",
    "Consider a setting where the feature and label space are $\\mathcal{X} = \\mathcal{Y} = [0, 1]$.    In this exercise we will consider both the square loss and the absolute loss, namely:\n",
    "\n",
    "$$ \\mathbb{l}_{sq}(y_1, y_2) = (y_1 - y_2)^2 $$\n",
    "$$ \\mathbb{l}_{abs}(y_1, y_2) = \\left \\vert y_1 - y_2 \\right \\vert $$\n",
    "\n",
    "\n",
    "Let (X, Y) be random, with the following with joint probability density $p_{XY}(x, y) = 2y$, where $x, y \\ \\in \\  [0, 1]$.  We define **statistical risk** as follows:\n",
    "\n",
    "**Definition (Statistical Risk)** For a prediction rule $f$ and a joint distribution of features and labels $P_{XY}$ the statistical risk $\\mathcal{R}(f)$ of $f$ is defined as \n",
    "\n",
    "$$\\mathcal{R}(f) \\equiv \\mathbb{E}_{XY}\\left[\\mathbb{l}(f(X),Y)\\ \\vert \\ f \\right]$$,\n",
    "\n",
    "where $(X, Y) \\sim P_{XY}$.  The conditional statement ensures the definition is sensible even if $f$ is a random quantity.\n",
    "\n",
    "\n",
    "2.1. Show that in this case $X$ and $Y$ are independent, meaning the feature $X$ carries no information about Y.\n",
    "\n",
    "2.2. What is the risk of prediction rule $f(x) = \\frac{1}{2}$ according to the two loss functions?\n",
    "\n",
    "2.3. What is the risk of the prediction rule $f^*(x) = \\frac{1}{\\sqrt{2}}$ according to the two loss functions?\n",
    "\n",
    "2.4. Show that $f^*$ has actually the smallest absolute loss risk among all prediction rules.\n",
    "\n",
    "**Hint (for 2.3):**\n",
    "\n",
    "    * In general the Bayes predictor according to the absolute value loss is the median of the conditional distribution of $Y$ given $X = x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Maxwell's Demon Has a Wonderful Way of showing us What Really Matters\n",
    "\n",
    "**Some Coding required**\n",
    "\n",
    "3.1. Find the entropy of the exponential probability density on support (0, $\\infty$) with mean $\\lambda$.\n",
    "\n",
    "3.2. Show that the exponential distribution $p^*$ is the maximum entropy distribution on support (0, $\\infty$)  with specified mean $\\lambda$. That is to say prove that for any continuous probability density function $p(x)$ on (0, $\\infty$) with mean $\\lambda$ then the entropy h(p) <= h($p^*$) with equality if and only if p is also the exponential with mean $\\lambda$\n",
    "\n",
    "\n",
    "We're familiar with the CLT as a way of approximating the sum of IID random variables with an appropriate Normal distribution.  Let's investigate this relationship by using the KL-Divergence.  Given n identically distributed Bernoulli variables $Y_i \\sim Bern(p)$, then their sum approaches a Normal distribution.\n",
    "\n",
    "3.3. Visualize this relationship by drawing n = 10,000 samples from a Bernoulli with p = 0.02.  These samples determine a random variable and thus a probability distribution (which in the last homework we called the empirical distribution of the data).  Visualize this probability distribution by plotting a normed histogram of the samples.  On your plot overlay the appropriately fitted Gaussian distribution.  Make sure to appropriately title and label your plot.\n",
    "\n",
    "3.4. From visual inspection are the two distributions close to each other?\n",
    "\n",
    "3.5. Formalize your answer to 3.3 and 3.4 by writing a program to compute the K-L divergence between the two distributions (the sum of 10000 sampled Bernoullis and the appropriate Gaussian).  What is the value of the KL divergence.\n",
    "\n",
    "3.6. Let's visualize the convergence of the sum of bernoulli RVs to a Gaussian as fortold by the CLT by repeating the process from 3.5 for various values of n.  We'll set our selection of sample sizes to the following: [100, 250, 500, 750, 1000, 2500, 5000, 7500, 10000, 50000, 100000].  Setting n to each of the specified sample sizes repeat the following procedure 10 times:\n",
    "\n",
    "* Draw n bernoulli samples using the Bernoulli parameter from 3.3 (p=0.02).\n",
    "* Calculate the Kullback-Leibler divergence between the random variable defined by the sum of Bernoullis samples and the appropriately fitted gaussian. \n",
    "\n",
    "For each sample size you should have 10 KL divergences. Construct a log scale (in both axes) plot of the Kullback-Leibler divergence and and the 3-$\\sigma$ envelope against the sample size.  What can you convergence of the distributions in question?  What does this mean for the CLT?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4: Marvel at the DC Flash Light Speed experiment\n",
    "\n",
    "Simon Newcomb did an experiment in 1882 to measure the speed of light. These are the times required for light to travel 7442 metres. These are recorded as deviations from 24,800 nanoseconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is in the following dataset $D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "light_speed = np.array([28, 26, 33, 24, 34, -44, 27, 16, 40, -2, 29, 22, 24, 21, 25,\n",
    "                        30, 23, 29, 31, 19, 24, 20, 36, 32, 36, 28, 25, 21, 28, 29,\n",
    "                        37, 25, 28, 26, 30, 32, 36, 26, 30, 22, 36, 23, 27, 27, 28,\n",
    "                        27, 31, 27, 26, 33, 26, 32, 32, 24, 39, 28, 24, 25, 32, 25,\n",
    "                        29, 27, 28, 29, 16, 23])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1. Plot a histogram of the data. Are there outliers in the data?  What data points might you consider to be outliers?\n",
    "\n",
    "4.2. We use a normal models with weakly informative priors to model this experiment. In particular assume uniform priors for both $\\mu$ and $\\sigma$:\n",
    "\n",
    "$$\\mu \\sim Uniform(0, 60)$$\n",
    "\n",
    "$$\\sigma \\sim Uniform(0.1, 50)$$\n",
    "\n",
    "Write down an expression for the posterior (joint) pdf $p(\\mu, \\sigma \\vert D)$.\n",
    "\n",
    "4.3. Set up a 500 point grid in both the $\\mu$ space and the $\\sigma$ space. Compute the normalized posterior on this grid and make a contour plot of it. \n",
    "\n",
    "**Hint: `np.meshgrid` is your friend**\n",
    "\n",
    "4.4. Use this normalized posterior to sample from the grid, posterior samples of size 500000. That is the posterior should be of shape `(500000, 2)`. (Hint: one way to do it is to first flatten the meshgrid into a grid of shape (250000, 2). Flatten the posterior probabilities as well into a size 250000 vector. Then sample 500000 indices and use them to index the grid). Plot the $\\mu$ and $\\sigma$ marginal posteriors.\n",
    "\n",
    "4.5. Experiment with reducing the grid size down to 100x100. How do the marginal posteriors now look? What does this look tell us about the dimensional scaling of this grid-sampling-in-proportion-to-posterior method of obtaining samples?\n",
    "\n",
    "4.6. Now draw from the data sampling normal distribution to obtain the posterior-predictive distribution. You will have as many samples as the size of the posterior. Plot the posterior predictive distribution against the data, and write down your observations.\n",
    "\n",
    "4.7. **Informally using a test-statistic**\n",
    "\n",
    "We might wish to compute a test statistic from the posterior predictive. Say for example, we wish to talk about the minimum value of the posterior predictive.\n",
    "\n",
    "The way to do this is to replicate the posterior predictive multiple times. We replicate the posterior-predictive (that is, do the sampling you did in 4.6) 66 times, which is the size of our dataset. In other words, we create as-many artificial datasets as there are samples in our posterior.\n",
    "\n",
    "This is called a **replicative posterior predictive**.\n",
    "\n",
    "Compute the replicative distribution of the minimum-value of the dataset and compare it to the actual value. What might you conclude about the quality of the specification of our model for the purposes of computing minimum values?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "nteract": {
   "version": "0.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
